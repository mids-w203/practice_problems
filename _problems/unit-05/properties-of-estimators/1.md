---
index: 1
caption: Upper Edge Estimation
statement: |
    Suppose that $X_1, X_2, ...$ is a sequence of iid random variables, each distributed uniform over $[0,a]$, where $a>0$ is an unknown parameter.  Define $M_n = \frac{2}{n} \sum_{i=1}^n X_i$. 

    a. Prove that each $M_n$ is an unbiased estimator for $a$.

    b. Prove that $\{M_n\}$ is a consistent estimator for $a$.
---

If an estimator is unbiased, then $E[\hat{\theta}] = \theta$. The proof then proceeds as follows: 

$$
\begin{aligned} 
    E[M_{(n)}] &= E[\frac{2}{n} \sum_{i=1}^{n} x_{i}] \\ 
        &= \frac{2}{n} E[\sum_{i=1}^{n}x_{i}] \\ 
        &= \frac{2}{n} \sum_{i=1}^{n} E[x_{i}] \\ 
        &= \frac{2}{n} \sum_{i=1}^{n} E[X]
\end{aligned} 
$$

At this point, we stop to ensure that we know $E[X]$. Because this is uniform, we know that the expectation has the following form: 

$$
\begin{aligned}
E[X] &= \int_{0}^{a} x f_{X}(x) dx \\
     &= \int_{0}^{a} x \frac{1}{a} dx \\ 
     &= \frac{1}{a} \int_{0}^{a} x dx \\ 
     &= \left. \frac{1}{2a} x^{2} \right|^{a}_{0} \\ 
     &= \frac{1}{2a} (a^{2} - 0^{2}) \\ 
     &= \frac{1}{2}a
\end{aligned}
$$

Now, we can substitute this $E[X]$ into our first set of statements. 

$$
\begin{aligned}
  E[M_{n}] &= \frac{2}{n} \sum_{i=1}^{n} E[X] \\
           &= \frac{2}{n} \sum_{i=1}^{n} \frac{1}{2}a \\
           &= \frac{2}{n} \cdot n \cdot \frac{1}{2}a \\
           &= \frac{2}{2} \cdot \frac{n}{n} \cdot a \\
           &= a
\end{aligned}
$$

And, so it is unbiased. 

To show that this is a consistent estimator, there are several options. If we remember the *CMT* then we can note that $M_{(n)} = 2 \overline{X}$, and that, since $\overline{X} \overset{p}\rightarrow E[X]$, $M_{n} \overset{p}\rightarrow 2E[X]$. Since $E[X] = \frac{1}{2}a$ we have shown convergence. 

An alternative method of showing this is to note that the estimator is unbiased, and so if we can show that its sampling variability gets very small, that it will meet the requirements of convergence in probability. 

We can write that the sampling variance of $M_{(n)} = V[M_{(n)}]$. 

$$
\begin{aligned}
  V[M_{(n)}] &= V[\frac{1}{n} \sum_{i=1}^{n}x_{i}] \\
             &= \frac{1}{n^{2}} V[\sum_{i=1}^{n}x_{i}] \\
             &= \frac{1}{n^{2}} \sum_{i=1}^{n} V[x_{i}] \\
             &= \frac{1}{n^{2}} \sum_{i=1}^{n} V[X] \\
\end{aligned}
$$

At this point, we need to establish $V[X]$. (See how using the CMT made this proof easier. This is an example for why proving pieces and having them in our back pockets can be useful.) Rather than deriving if from whole cloth, we'll note that it is well know that the variance of a uniform distribution of this form is $V[X] = \frac{1}{12}a^{2}$. 

Returning to the statement before: 

$$
\begin{aligned}
  V[M_{(n)}] &= \frac{1}{n^{2}} \sum_{i=1}^{n} V[X] \\
  &= \frac{1}{n} \cdot \frac{1}{12}a^{2}
\end{aligned}
$$

And so, as $n \rightarrow \infty$, we note that this quantity approaches zero. 
