---
index: 5
answer: "Regression: Origins"
statement: |
    Consider random variables $X$ and $Y$.  In the technique called *regression through the origin*, we are interested in linear predictors of the form,

    $$g(x) = b_1 x$$

    In other words, linear predictors that pass through the origin.  Given such a predictor, define $\epsilon = Y - g(X)$ as always.  We are interested in minimizing mean squared error:

    $$\beta_1 = \text{argmin}_{b_1} E[\epsilon^2]$$


    Examine the proof on page 77 of *Agnostic Statistics* and consider how it would be different for regression through the origin.

    1. Prove that $E[\epsilon X] = 0$ as before.
    2. Is it still true that $Cov[\epsilon, X] = 0$?  Prove it or give a counterexample.
    3. Compute an expression for $\beta_1$.
---

**Part 1**

The first part of the proof is almost exactly the same as that in the textbook, except that there is only one first-order condition, not two. The optimization is,

$\beta_1 = \text{argmin}_{b_1} E[\epsilon^2]$

The single first-order condition is therefore,

$0 = \frac{\partial E[\epsilon^2]}{\partial \beta_1}$

The rest of the computation is the same:

$0 = \frac{\partial E[\epsilon^2]}{\partial \beta_1} = E\left[ \frac{\partial \epsilon^2]}{\partial \beta_1} \right] = E\left[ 2\epsilon \frac{\partial \epsilon}{\partial \beta_1} \right] = -2E[\epsilon X]$

**Part 2**

Is it still true that $Cov[\epsilon, X] = 0$? The answer is no. The best way to discover this is to try drawing some simple distributions for which the regression line is very easy to write down. For example, consider the discrete distribution with probability mass function $f$ given by,

$$f(x,y) = \begin{cases}1/2, &(x,y) \in \\{ (1,0), (0,1) \\}\\\\ 0 &otherwise\end{cases}$$

We can compute the following: 

$$
E[XY] = \sum_{(x,y) \in Supp[X,Y]} xy f(x,y) = 1 \cdot 0 \cdot \frac{1}{2} +  0 \cdot 1 \cdot \frac{1}{2} = 0
$$


Plugging into the slope of the line,

$$\beta_1 = \frac{E[XY]}{E[X^2]} = 0$$

We find that the regression line has zero slope. The covariance can then be computed as follows:

$$Cov[\epsilon, X] = Cov[Y - 0 X, X] = Cov[Y,X] = E[XY] - E[X]E[Y] = 0 - \frac{1}{2}\frac{1}{2}= - \frac{1}{4}$$

Thus, we note that the covariance is not zero.

**Part 3**

$$
\begin{align}
  \frac{d E[\epsilon^{2}]}{d\beta} &= E[2\epsilon (-X)] \\ 
    &= -2 E[\epsilon X] \\ 
    &= -2 E[(Y - b_1 X)X] \\ 
    &= -2 E[XY - b_1 X^2] \\ 
    &= -2 E[EY] - b_1 E[X^2] = 0 \\
\beta_1 &= \frac{E[XY]}{E[X^2]}
\end{align}
$$
