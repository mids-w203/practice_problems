---
index: 5
answer: "Regression: Origins"
statement: |
    Consider random variables $X$ and $Y$.  In the technique called *regression through the origin*, we are interested in linear predictors of the form,

    $$g(x) = b_1 x$$

    In other words, linear predictors that pass through the origin.  Given such a predictor, define $\epsilon = Y - g(X)$ as always.  We are interested in minimizing mean squared error:

    $$\beta_1 = \text{argmin}_{b_1} E[\epsilon^2]$$


    Examine the proof on page 77 of *Agnostic Statistics* and consider how it would be different for regression through the origin.

    1. Prove that $E[\epsilon X] = 0$ as before.
    2. Is it still true that $Cov[\epsilon, X] = 0$?  Prove it or give a counterexample.
    3. Compute an expression for $\beta_1$.
---

# Part 1

We want to choose a slope $b_1$ that reduces expected squared error:

$$\beta_1 = \text{argmin}_{b_1} E[\epsilon^2]$$

Note that the optimization now is only in terms of one variable, $b_1$, so we take a derivative wrt that variable, and set it equal to zero. (We are going to ignore the second order conditions in this proof because we've got tests to grade... ðŸ˜‚)

The first line below is going to use the chain rule. 

$$
\begin{aligned}
  \frac{d E[\epsilon^{2}]}{d\beta} &= E[2\epsilon (-X)] \\ 
    &= -2 E[\epsilon X] \\ 
    &= -2 E[(Y - b_1 X)X] \\ 
    &= -2 E[XY - b_1 X^2] \\ 
    &= -2 E[EY] - b_1 E[X^2] = 0 \\
\beta_1 &= \frac{E[XY]}{E[X^2]}
\end{aligned}
$$

With this statement for $\beta_1$ established, we can turn attention to the statement on hand, namely, does $E[\epsilon X] = 0$? 

$$
\begin{aligned} 
E[\epsilon X] &= E[(Y - \beta X)X] \\  &= E[(Y - \frac{E[XY]}{E[X^2]}X)X] \\   &= E[XY] - E[\frac{E[XY]}{E[X^2]}X^2] \\  &= E[XY] - \frac{E[XY]}{E[X^2]}E[X^2] \\  &= E[XY] - E[XY] \\  &= 0
\end{aligned} 
$$

# Part 2

Is it still true that $Cov[\epsilon, X] = 0$? The answer is no. The best way to discover this is to try drawing some simple distributions for which the regression line is very easy to write down. For example, consider the discrete distribution with probability mass function $f$ given by,

$$
f(x,y) = \begin{cases}1/2, &(x,y) \in \{ (1,0), (0,1) \}\\ 0 &otherwise\end{cases}
$$

We can compute the following: 

$$
E[XY] = \sum_{(x,y) \in Supp[X,Y]} xy f(x,y) = 1 \cdot 0 \cdot \frac{1}{2} +  0 \cdot 1 \cdot \frac{1}{2} = 0
$$

Plugging into the slope of the line,

$$\beta_1 &= \frac{E[XY]}{E[X^2]} = 0$$

We find that the regression line has zero slope. The covariance can then be computed as follows:

$$Cov[\epsilon, X] = Cov[Y - 0 X, X] = Cov[Y,X] = E[XY] - E[X]E[Y] = 0 - \frac{1}{2}\frac{1}{2}= - \frac{1}{4}$$

Thus, we note that the covariance is not zero.
