---
index: 5
answer: "Regression: Origins"
statement: |
    Consider random variables $X$ and $Y$.  In the technique called *regression through the origin*, we are interested in linear predictors of the form,

    $$g(x) = b_1 x$$

    In other words, linear predictors that pass through the origin.  Given such a predictor, define $\epsilon = Y - g(X)$ as always.  We are interested in minimizing mean squared error:

    $$\beta_1 = \text{argmin}_{b_1} E[\epsilon^2]$$


    Examine the proof on page 77 of *Agnostic Statistics* and consider how it would be different for regression through the origin.

    1. Prove that $E[\epsilon X] = 0$ as before.
    2. Is it still true that $Cov[\epsilon, X] = 0$?  Prove it or give a counterexample.
    3. Compute an expression for $\beta_1$.
---

Oh, this is a fun one! And there is a lot that we can show! 

# Part 1

We are trying to prove that $E[\epsilon X] = 0$. Well, before we can get going on this, we actually have to develop the statement for the $\beta$ that we generate with the new requirement that the line pass through the origin. This is a new optimization problem: 

$$
\underset{\beta}{argmin}\ E[\epsilon^2]
$$

To do so, we note that the optimization now is only in terms of one variable, $\beta$, so we take derivatives wrt that varaible, and set equal to zero. (We are going to ignore the second order conditions in this proof because we've got tests to grade... ðŸ˜‚

The first line below is going to use the chain rule. 

$$
\begin{aligned}
  \frac{d E[\epsilon^{2}]}{d\beta} &= E[2\epsilon (-X)] \\ 
    &= -2 E[\epsilon X] \\ 
    &= -2 E[(Y - \beta X)X] \\ 
    &= -2 E[XY - \beta X^2] \\ 
    &= -2 E[EY] - beta E[X^2] = 0 \\
\beta &= \frac{E[XY]}{E[X^2]}
\end{aligned}
$$

With this statement for $\beta$ established, we can turn attention to the statement on hand, namely, does $E[\epsilon X] = 0$? 

$$
\begin{aligned} 
E[\epsilon X] &= E[(Y - \beta X)X] \\ 
  &= E[(Y - \frac{E[XY]}{E[X^2]}X)X] \\ 
  &= E[XY] - E[\frac{E[XY]}{E[X^2]}X^2] \\
  &= E[XY] - \frac{E[XY]}{E[X^2]}E[X^2] \\
  &= E[XY] - E[XY] \\ 
  &= 0
\end{aligned} 
$$

# Part 2

Is it still true that $Cov[\epsilon, X] = 0$?

We've just shown that $E[\epsilon X] = 0$, so we substitute below: 

$$
\begin{aligned} 
  Cov[X, \epsilon] &= E[X\epsilon] - E[X]E[\epsilon] \\ 
   &= 0 - E[X]E[\epsilon] \\
\end{aligned} 
$$

There are a few ways for this statement to be zero: 

- $E[X] = 0$, 
- $E[\epsilon] = 0$, 
- Or the product of the two is zero. 

We note, first, that there is no requirement for $E[X] = 0$. Let's turn attention to $E[\epsilon]$. 

$$
\begin{aligned} 
E[\epsilon] &= E[Y - \beta X] \\ 
  &= E[Y] - \beta E[X] \\ 
  &= E[Y] - \frac{E[XY]}{E[X^2]} E[X] \\ 
\end{aligned}
$$

From here, there is no obvious requirement that this will be zero. As we're writing the solution, it feels less and less like this is going to go! So, we're switching to start to think of a counter example. 

Suppose that is is distributd non-linearly according to the PDF $f_{XY}(x,y) = \frac{3}{32} x^2y$ . 

With this function, we can develop a statement for $\beta$: 

$$
\begin{aligned} 
  \beta &= \frac{E[XY]}{E[X^2]} \\
\end{aligned} 
$$

We're going to solve for these quantites on the side, noting that $E[XY] = \frac{1}{2}$, and $E[X^2] = \frac{3}{5}$, so $\beta = \frac{1}{5}$. 

$$
\begin{aligned} 
  Cov(\epsilon, X) &= E[X\epsilon] - E[X]E[\epsilon] \\ 
    &= 0 - (\frac{3}{4})(E[\epsilon]) \\ 
    &= 0 - \frac{3}{4}(E[Y - \frac{1}{5}X]) \\ 
    &= 0 - \frac{3}{4}(E[Y] - \frac{1}{5}E[X]) \\ 
    &= 0 - \frac{3}{4}(E[Y] - \frac{1}{5}\frac{3}{4}) \\ 
0   &\neq 0 - \frac{3}{4}(\frac{1}{3} - \frac{3}{20}) \\
\end{aligned} 
$$
