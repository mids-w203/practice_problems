---
index: 5
answer: "Regression: Origins"
statement: |
    Consider random variables $X$ and $Y$.  In the technique called *regression through the origin*, we are interested in linear predictors of the form,

    $$g(x) = b_1 x$$

    In other words, linear predictors that pass through the origin.  Given such a predictor, define $\epsilon = Y - g(X)$ as always.  We are interested in minimizing mean squared error:

    $$\beta_1 = \text{argmin}_{b_1} E[\epsilon^2]$$


    Examine the proof on page 77 of *Agnostic Statistics* and consider how it would be different for regression through the origin.

    1. Prove that $E[\epsilon X] = 0$ as before.
    2. Is it still true that $Cov[\epsilon, X] = 0$?  Prove it or give a counterexample.
    3. Compute an expression for $\beta_1$.
---
